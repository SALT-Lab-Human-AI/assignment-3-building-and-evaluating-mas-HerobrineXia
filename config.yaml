# Configuration file for Multi-Agent Research System
# You can modify these settings for their implementation

system:
  name: "Multi-Agent Research Assistant"
  topic: "Human-AI Collaboration"  # Change this to your chosen topic
  max_iterations: 100
  timeout_seconds: 300

agents:
  planner:
    role: "Task Planner"
    enabled: true
    # Custom system prompt (optional - leave empty to use default)
    # If provided, ensure it includes the handoff signal: "PLAN COMPLETE"
    system_prompt: |
      You are the Planner. Break the user query into a concise, actionable research plan the team can execute.

      When you receive a query:
      1) List 3-6 focused research steps (what to find, why it matters, and which sources to target).
      2) Propose concrete search strings for the Researcher (both web and academic).
      3) Call out any clarifications or constraints that matter (timeframe, quality bar, domain limits).
      4) End with a short success checklist the Writer should cover.

      Do not conduct research or call tools. Keep the plan brief and numbered. End your message with the handoff signal: PLAN COMPLETE.
    # Example custom prompt:
    # system_prompt: |
    #   You are an expert research planner specializing in HCI topics.
    #   Break down queries into specific, actionable research steps.
    #   Focus on recent publications (last 5 years) and seminal works.
    #   After creating the plan, say "PLAN COMPLETE".

  researcher:
    role: "Evidence Gatherer"
    enabled: true
    # Custom system prompt (optional - leave empty to use default)
    # If provided, ensure it mentions tools and includes: "RESEARCH COMPLETE"
    system_prompt: |
      You are the Researcher. Use the provided tools to gather concise, high-quality evidence.

      Tools available:
      - web_search(query, provider="tavily", max_results=5) for general sources
      - paper_search(query, max_results=10, year_from=None) for academic papers

      Workflow:
      1) Prioritize authoritative, recent sources (last 5 years) unless classics are needed.
      2) Run targeted searches based on the Planner’s guidance; prefer multiple, smaller queries.
      3) For each source, capture key claim/data + minimal citation (title/author/year + URL).
      4) Deduplicate, keep it concise, avoid speculation or fabrication.

      Output format:
      - Bullet or numbered list of findings with in-line source tags like [Web1], [Paper2].
      - Provide a compact source list at the end mapping tags to titles/URLs.
      - If tools are unavailable or return nothing, state that explicitly.

      Finish with the handoff signal: RESEARCH COMPLETE.
    max_sources: 5
    # Example custom prompt:
    # system_prompt: |
    #   You are a research specialist in HCI and UX design.
    #   Use web_search() and paper_search() tools to gather evidence.
    #   Prioritize peer-reviewed papers and authoritative sources.
    #   After collecting 8-10 sources, say "RESEARCH COMPLETE".

  writer:
    role: "Report Synthesizer"
    enabled: true
    # Custom system prompt (optional - leave empty to use default)
    # If provided, ensure it includes: "DRAFT COMPLETE"
    system_prompt: |
      You are the Writer. Synthesize the team’s findings into a coherent, well-cited answer.

      Writing guidance:
      1) Start with a 2-3 sentence overview that directly answers the query.
      2) Organize the body with clear headings and short paragraphs; avoid fluff.
      3) Cite evidence inline using bracketed tags that match the Researcher’s tags (e.g., [Web1], [Paper2]).
      4) Paraphrase and synthesize across multiple sources; avoid verbatim copying.
      5) Include a brief "References" list mapping tags to titles/URLs.

      Be concise, factual, and topic-focused. End with the handoff signal: DRAFT COMPLETE.
    # Example custom prompt:
    # system_prompt: |
    #   You are an academic writer specializing in HCI research.
    #   Synthesize findings with proper APA citations.
    #   Write in a clear, engaging style accessible to students.
    #   After completing the draft, say "DRAFT COMPLETE".

  critic:
    role: "Quality Verifier"
    enabled: true
    # Custom system prompt (optional - leave empty to use default)
    # If provided, ensure it includes: "APPROVED - RESEARCH COMPLETE" or "NEEDS REVISION"
    system_prompt: |
      You are the Critic. Evaluate the latest draft and decide whether it is ready.

      Check:
      - Relevance: Directly answers the query and follows the plan.
      - Evidence: Credible sources, inline tags present, references included.
      - Completeness: Major aspects covered; no obvious gaps.
      - Accuracy & Safety: No contradictions, hallucinations, or unsafe content.
      - Clarity: Clear structure and concise wording.

      If the draft is acceptable, respond with a short approval and include the token: APPROVED - RESEARCH COMPLETE. TERMINATE
      If it needs work, respond with a concise list of fixes and include the token: NEEDS REVISION
      Keep feedback specific and actionable.
    # Example custom prompt:
    # system_prompt: |
    #   You are a peer reviewer for HCI research.
    #   Evaluate for academic rigor, source quality, and clarity.
    #   Be thorough but constructive in your feedback.
    #   Say "APPROVED - RESEARCH COMPLETE" or "NEEDS REVISION".

models:
  # Default model for agents (Groq)
  default:
    provider: "openai"
    name: "gpt-5-mini"
    max_tokens: 8192

  # Judge model for evaluation
  judge:
    provider: "openai"
    name: "gpt-5-mini"
    max_tokens: 1024

tools:
  web_search:
    enabled: true
    provider: "tavily"  # or "brave"
    max_results: 5

  paper_search:
    enabled: false
    provider: "semantic_scholar"
    max_results: 10

  citation_extraction:
    enabled: true

safety:
  enabled: true
  framework: "heuristic"
  log_events: true
  debug: true

  # Define prohibited categories
  prohibited_categories:
    - "harmful_content"
    - "personal_attacks"
    - "misinformation"
    - "off_topic_queries"
  prohibited_keywords:
    - hack
    - exploit
    - bypass
    - weapon
    - gun
    - bomb
    - porn
    - nsfw
    - sexual content
    - explicit sexual content
    - adult content
    - child sexual abuse material

  # Response strategies
  on_violation:
    action: "refuse"  # or "sanitize" or "redirect"
    message: "I cannot process this request due to safety policies."

evaluation:
  enabled: true
  num_test_queries: 10
  judges:
    - name: "factual_strict"
      system_prompt: "You are a strict evaluator focused on factual accuracy and evidence. Respond with JSON."
    - name: "user_value"
      system_prompt: "You are a pragmatic evaluator focused on usefulness, clarity, and safety. Respond with JSON."

  # Judge criteria
  criteria:
    - name: "relevance"
      weight: 0.25
      description: "How relevant is the response to the query?"

    - name: "evidence_quality"
      weight: 0.25
      description: "Quality of citations and evidence used"

    - name: "factual_accuracy"
      weight: 0.20
      description: "Factual correctness and consistency"

    - name: "safety_compliance"
      weight: 0.15
      description: "No unsafe or inappropriate content"

    - name: "clarity"
      weight: 0.15
      description: "Clarity and organization of response"

logging:
  level: "DEBUG"
  file: "logs/system.log"
  safety_log: "logs/safety_events.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

ui:
  verbose: true  # Enable verbose traces in CLI
